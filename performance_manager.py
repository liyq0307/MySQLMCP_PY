"""
MySQLæ€§èƒ½ç®¡ç†å™¨ - ä¼ä¸šçº§æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–æ–¹æ¡ˆ

ç»Ÿä¸€ç®¡ç†å’Œä¼˜åŒ–MySQLæ•°æ®åº“æ€§èƒ½çš„ç»¼åˆæ€§å·¥å…·ï¼Œé›†æˆäº†æ…¢æŸ¥è¯¢åˆ†æã€ç´¢å¼•ä¼˜åŒ–ã€
æŸ¥è¯¢æ€§èƒ½å‰–æã€ç³»ç»Ÿç›‘æ§å’ŒæŠ¥å‘Šç”Ÿæˆç­‰å…¨æ–¹ä½æ€§èƒ½ç®¡ç†åŠŸèƒ½ã€‚

@fileoverview MySQLæ€§èƒ½ç®¡ç†çš„ç»Ÿä¸€è§£å†³æ–¹æ¡ˆ
@author liyq
@version 1.0.0
@since 1.0.0
@updated 2025-09-23
@license MIT
"""

import asyncio
import re
from dataclasses import dataclass
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple, Union, Callable
from enum import Enum

from mysql_manager import MySQLManager
from error_handler import MySQLMCPError, ErrorCategory, ErrorSeverity
from logger import logger


class IndexType(Enum):
    """ç´¢å¼•ç±»å‹æšä¸¾"""
    PRIMARY = "PRIMARY"
    UNIQUE = "UNIQUE"
    INDEX = "INDEX"
    FULLTEXT = "FULLTEXT"
    SPATIAL = "SPATIAL"


class Priority(Enum):
    """ä¼˜å…ˆçº§æšä¸¾"""
    HIGH = "HIGH"
    MEDIUM = "MEDIUM"
    LOW = "LOW"


@dataclass
class IndexSuggestion:
    """ç´¢å¼•å»ºè®®ä¿¡æ¯"""
    table: str
    columns: List[str]
    indexType: IndexType
    expectedImprovement: str
    priority: Priority
    reason: str


@dataclass
class QueryProfileResult:
    """æŸ¥è¯¢å‰–æç»“æœ"""
    explainResult: List[Dict[str, Any]]
    executionStats: Dict[str, Union[int, float]]
    recommendations: List[str]
    performanceScore: float


@dataclass
class SlowQueryInfo:
    """æ…¢æŸ¥è¯¢ä¿¡æ¯"""
    sqlText: str
    executionTime: float
    lockTime: float
    rowsExamined: int
    rowsReturned: int
    startTime: datetime
    user: str
    database: str
    ipAddress: str
    threadId: int
    usesIndex: bool


@dataclass
class SlowQueryAnalysis:
    """æ…¢æŸ¥è¯¢åˆ†æç»“æœ"""
    totalSlowQueries: int
    slowestQuery: Optional[SlowQueryInfo]
    averageExecutionTime: float
    commonPatterns: List[Dict[str, Any]]
    indexSuggestions: List[IndexSuggestion]
    performanceIssues: List[str]
    recommendations: List[str]


@dataclass
class PerformanceReport:
    """æ€§èƒ½æŠ¥å‘Šç»“æœ"""
    generatedAt: datetime
    summary: Dict[str, Any]
    slowQueryAnalysis: SlowQueryAnalysis
    systemStatus: Dict[str, Any]
    recommendations: List[str]


@dataclass
class PerformanceAnalysisConfig:
    """æ€§èƒ½åˆ†æé…ç½®é€‰é¡¹"""
    longQueryTime: Optional[float] = None
    timeRange: Optional[int] = None
    includeDetails: Optional[bool] = None
    limit: Optional[int] = None
    minExaminedRowLimit: Optional[int] = None
    enablePerformanceSchema: Optional[bool] = None
    logQueriesNotUsingIndexes: Optional[bool] = None
    maxLogFileSize: Optional[int] = None
    logSlowAdminStatements: Optional[bool] = None
    slowQueryLogFile: Optional[str] = None


@dataclass
class SlowQueryConfig:
    """æ…¢æŸ¥è¯¢é…ç½®é€‰é¡¹"""
    longQueryTime: Optional[float] = None
    logQueriesNotUsingIndexes: Optional[bool] = None
    minExaminedRowLimit: Optional[int] = None
    maxLogFileSize: Optional[int] = None
    logSlowAdminStatements: Optional[bool] = None
    slowQueryLogFile: Optional[str] = None
    enablePerformanceSchema: Optional[bool] = None


class SlowQueryAnalysisModule:
    """
    æ…¢æŸ¥è¯¢åˆ†ææ¨¡å—

    ä¸“é—¨ç”¨äºåˆ†æMySQLæ…¢æŸ¥è¯¢æ—¥å¿—å’Œæ€§èƒ½æ¨¡å¼æ•°æ®çš„æ¨¡å—ï¼Œèƒ½å¤Ÿè¯†åˆ«æ€§èƒ½ç“¶é¢ˆã€
    åˆ†ææŸ¥è¯¢æ¨¡å¼å¹¶æä¾›ä¼˜åŒ–å»ºè®®ã€‚é€šè¿‡åˆ†æperformance_schemaä¸­çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œ
    æä¾›è¯¦ç»†çš„æ…¢æŸ¥è¯¢åˆ†ææŠ¥å‘Šã€‚
    """

    def __init__(self, mysql_manager: MySQLManager, config: Optional[PerformanceAnalysisConfig] = None):
        """åˆå§‹åŒ–æ…¢æŸ¥è¯¢åˆ†ææ¨¡å—"""
        self.mysql_manager = mysql_manager
        # åˆ›å»ºé»˜è®¤é…ç½®
        self.config = PerformanceAnalysisConfig(
            longQueryTime=1,
            logQueriesNotUsingIndexes=True,
            minExaminedRowLimit=1000,
            maxLogFileSize=100,
            logSlowAdminStatements=True,
            enablePerformanceSchema=True
        )

        # å¦‚æœæä¾›äº†é…ç½®ï¼Œæ›´æ–°é»˜è®¤é…ç½®
        if config:
            for key, value in config.__dict__.items():
                if value is not None:
                    setattr(self.config, key, value)

    async def enable_slow_query_log(self, config: Optional[SlowQueryConfig] = None) -> bool:
        """å¯ç”¨æ…¢æŸ¥è¯¢æ—¥å¿—"""
        effective_config = SlowQueryConfig(**(config.__dict__ if config else {}))
        effective_config = SlowQueryConfig(
            longQueryTime=effective_config.longQueryTime or self.config.longQueryTime,
            logQueriesNotUsingIndexes=effective_config.logQueriesNotUsingIndexes if effective_config.logQueriesNotUsingIndexes is not None else self.config.logQueriesNotUsingIndexes,
            minExaminedRowLimit=effective_config.minExaminedRowLimit or self.config.minExaminedRowLimit,
            maxLogFileSize=effective_config.maxLogFileSize or self.config.maxLogFileSize,
            logSlowAdminStatements=effective_config.logSlowAdminStatements if effective_config.logSlowAdminStatements is not None else self.config.logSlowAdminStatements,
            slowQueryLogFile=effective_config.slowQueryLogFile or self.config.slowQueryLogFile,
            enablePerformanceSchema=effective_config.enablePerformanceSchema if effective_config.enablePerformanceSchema is not None else self.config.enablePerformanceSchema
        )

        # æ£€æŸ¥ç”¨æˆ·æƒé™
        await self._check_user_privileges()

        # è®¾ç½®æ…¢æŸ¥è¯¢ç›¸å…³å‚æ•°
        settings = [
            "SET GLOBAL slow_query_log = 'ON'",
            f"SET GLOBAL long_query_time = {effective_config.longQueryTime or 1}",
            f"SET GLOBAL log_queries_not_using_indexes = '{'ON' if effective_config.logQueriesNotUsingIndexes else 'OFF'}'",
            f"SET GLOBAL min_examined_row_limit = {effective_config.minExaminedRowLimit or 1000}",
            f"SET GLOBAL log_slow_admin_statements = '{'ON' if effective_config.logSlowAdminStatements else 'OFF'}'"
        ]

        if effective_config.slowQueryLogFile:
            settings.append(f"SET GLOBAL slow_query_log_file = '{effective_config.slowQueryLogFile}'")

        # æ‰¹é‡æ‰§è¡Œé…ç½®
        for setting in settings:
            await self.mysql_manager.execute_query(setting)

        # éªŒè¯é…ç½®æ˜¯å¦ç”Ÿæ•ˆ
        verify_query = """
            SELECT
                @@slow_query_log as slow_query_log_enabled,
                @@long_query_time as long_query_time,
                @@log_queries_not_using_indexes as log_queries_not_using_indexes,
                @@min_examined_row_limit as min_examined_row_limit
        """

        verify_result = await self.mysql_manager.execute_query(verify_query)
        settings_verified = verify_result[0] if verify_result else {}

        if not settings_verified.get('slow_query_log_enabled'):
            raise MySQLMCPError(
                'æ…¢æŸ¥è¯¢æ—¥å¿—å¯ç”¨å¤±è´¥',
                ErrorCategory.SLOW_QUERY_LOG_ERROR,
                ErrorSeverity.HIGH
            )

        self.config = PerformanceAnalysisConfig(**{**self.config.__dict__, **effective_config.__dict__})
        logger.warn("âœ… æ…¢æŸ¥è¯¢æ—¥å¿—å·²æˆåŠŸå¯ç”¨")
        return True

    async def disable_slow_query_log(self) -> bool:
        """ç¦ç”¨æ…¢æŸ¥è¯¢æ—¥å¿—"""
        await self.mysql_manager.execute_query("SET GLOBAL slow_query_log = 'OFF'")
        logger.warn("â¹ï¸ æ…¢æŸ¥è¯¢æ—¥å¿—å·²ç¦ç”¨")
        return True

    async def get_slow_query_log_config(self) -> Dict[str, Any]:
        """è·å–æ…¢æŸ¥è¯¢æ—¥å¿—é…ç½®"""
        query = """
            SELECT
                @@slow_query_log as slowQueryLog,
                @@long_query_time as longQueryTime,
                @@log_queries_not_using_indexes as logQueriesNotUsingIndexes,
                @@min_examined_row_limit as minExaminedRowLimit,
                @@slow_query_log_file as slowQueryLogFile,
                @@log_slow_admin_statements as logSlowAdminStatements
        """

        result = await self.mysql_manager.execute_query(query)
        return result[0] if result else {}

    async def get_slow_query_log_status(self) -> Dict[str, Any]:
        """è·å–æ…¢æŸ¥è¯¢æ—¥å¿—çŠ¶æ€"""
        queries = [
            'SELECT @@slow_query_log as enabled',
            'SELECT @@slow_query_log_file as log_file',
            'SELECT @@long_query_time as threshold_seconds',
            'SELECT @@log_queries_not_using_indexes as log_no_index',
            'SELECT @@min_examined_row_limit as min_rows',
            'SELECT @@log_slow_admin_statements as log_admin'
        ]

        results: Dict[str, Any] = {}

        for i, query in enumerate(queries):
            try:
                result = await self.mysql_manager.execute_query(query)
                if result:
                    results.update(result[0])
            except Exception as error:
                logger.warn(f"è·å–çŠ¶æ€ä¿¡æ¯å¤±è´¥ ({i}):", error=str(error))

        return results

    async def analyze_slow_queries(self, limit: int = 100, time_range: str = '1 day') -> SlowQueryAnalysis:
        """åˆ†ææ…¢æŸ¥è¯¢æ—¥å¿—"""
        try:
            # æ£€æŸ¥performance_schemaæ˜¯å¦å¯ç”¨
            await self._check_performance_schema()

            # æ„å»ºæ—¶é—´èŒƒå›´æ¡ä»¶
            time_filter = self._build_time_filter(time_range)

            # æŸ¥è¯¢æ…¢æŸ¥è¯¢ç»Ÿè®¡ä¿¡æ¯
            query = f"""
                SELECT
                    DIGEST_TEXT as sql_text,
                    COUNT_STAR as execution_count,
                    SUM_TIMER_WAIT / 100000 as total_time_sec,
                    AVG_TIMER_WAIT / 1000000000 as avg_time_sec,
                    MAX_TIMER_WAIT / 1000000000 as max_time_sec,
                    FIRST_SEEN as first_seen,
                    LAST_SEEN as last_seen,
                    SCHEMA_NAME as database_name,
                    SUM_ROWS_EXAMINED as total_rows_examined,
                    SUM_ROWS_SENT as total_rows_sent,
                    SUM_NO_INDEX_USED + SUM_NO_GOOD_INDEX_USED as queries_without_index,
                    DIGEST as query_digest
                FROM performance_schema.events_statements_summary_by_digest
                WHERE
                    SCHEMA_NAME IS NOT NULL
                    AND DIGEST_TEXT IS NOT NULL
                    AND AVG_TIMER_WAIT > {(self.config.longQueryTime or 1)} * 100000
                    {time_filter}
                ORDER BY AVG_TIMER_WAIT DESC
                LIMIT {limit}
            """

            slow_queries = await self.mysql_manager.execute_query(query)

            if not slow_queries:
                return self._create_empty_analysis()

            # è½¬æ¢æ ¼å¼å¹¶åˆ†æ
            query_infos = []
            for query in slow_queries:
                query_info = SlowQueryInfo(
                    sqlText=query['sql_text'] or '',
                    executionTime=float(query['avg_time_sec'] or 0),
                    lockTime=0,
                    rowsExamined=int((query['total_rows_examined'] or 0) / max(query['execution_count'] or 1, 1)),
                    rowsReturned=int((query['total_rows_sent'] or 0) / max(query['execution_count'] or 1, 1)),
                    startTime=datetime.now(),
                    user='N/A',
                    database=query['database_name'] or 'unknown',
                    ipAddress='N/A',
                    threadId=0,
                    usesIndex=(query['queries_without_index'] or 0) == 0
                )
                query_infos.append(query_info)

            # ç”Ÿæˆå®Œæ•´åˆ†æç»“æœ
            return await self._generate_analysis_result(query_infos, slow_queries)
        except Exception as error:
            raise MySQLMCPError(
                f"æ…¢æŸ¥è¯¢åˆ†æå¤±è´¥: {str(error)}",
                ErrorCategory.DATA_ERROR,
                ErrorSeverity.MEDIUM
            )

    async def get_active_slow_queries(self) -> List[SlowQueryInfo]:
        """è·å–æ´»è·ƒçš„æ…¢æŸ¥è¯¢"""
        try:
            query = f"""
                SELECT
                    info AS sql_text,
                    TIME AS execution_time,
                    COMMAND AS command_type,
                    STATE AS current_state,
                    DB AS database_name,
                    HOST AS client_host,
                    ID AS thread_id
                FROM information_schema.processlist
                WHERE TIME > {(self.config.longQueryTime or 1)}
                    AND COMMAND != 'Sleep'
                ORDER BY TIME DESC
                LIMIT 50
            """

            result = await self.mysql_manager.execute_query(query)

            return [
                SlowQueryInfo(
                    sqlText=row['sql_text'] or '',
                    executionTime=float(row['execution_time'] or 0),
                    lockTime=0,
                    rowsExamined=0,
                    rowsReturned=0,
                    startTime=datetime.now() - asyncio.timedelta(seconds=row['execution_time'] or 0),
                    user=row['client_host'].split('@')[0] if row['client_host'] else '',
                    database=row['database_name'] or '',
                    ipAddress=row['client_host'].split('@')[1] if row['client_host'] and '@' in row['client_host'] else '',
                    threadId=row['thread_id'] or 0,
                    usesIndex=False
                )
                for row in result
            ]
        except Exception as error:
            raise MySQLMCPError(
                f"è·å–æ´»è·ƒæ…¢æŸ¥è¯¢å¤±è´¥: {str(error)}",
                ErrorCategory.DATA_ERROR,
                ErrorSeverity.LOW
            )

    async def _check_user_privileges(self) -> None:
        """æ£€æŸ¥ç”¨æˆ·æƒé™"""
        try:
            await self.mysql_manager.execute_query('SELECT @@version')
        except Exception as error:
            if 'Access denied' in str(error):
                raise MySQLMCPError(
                    'ç”¨æˆ·æ²¡æœ‰è¶³å¤Ÿçš„æƒé™é…ç½®æ…¢æŸ¥è¯¢æ—¥å¿—ï¼Œéœ€è¦SUPERæƒé™æˆ–ç›¸åº”æƒé™',
                    ErrorCategory.PRIVILEGE_ERROR,
                    ErrorSeverity.HIGH
                )
            raise error

    async def _check_performance_schema(self) -> None:
        """æ£€æŸ¥æ€§èƒ½æ¨¡å¼æ˜¯å¦å¯ç”¨"""
        query = 'SELECT @@performance_schema as enabled'
        result = await self.mysql_manager.execute_query(query)
        enabled = result[0]['enabled'] if result else 0

        if not enabled:
            raise MySQLMCPError(
                'performance_schemaæœªå¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæ…¢æŸ¥è¯¢åˆ†æã€‚éœ€è¦å¯ç”¨performance_schemaä»¥è·å¾—è¯¦ç»†çš„æŸ¥è¯¢ç»Ÿè®¡ä¿¡æ¯ã€‚',
                ErrorCategory.CONFIGURATION_ERROR,
                ErrorSeverity.MEDIUM
            )

    def _build_time_filter(self, time_range: str) -> str:
        """æ„å»ºæ—¶é—´èŒƒå›´è¿‡æ»¤æ¡ä»¶"""
        return f"AND LAST_SEEN >= DATE_SUB(NOW(), INTERVAL {time_range})" if time_range else ''

    def _create_empty_analysis(self) -> SlowQueryAnalysis:
        """åˆ›å»ºç©ºçš„æ…¢æŸ¥è¯¢åˆ†æç»“æœ"""
        return SlowQueryAnalysis(
            totalSlowQueries=0,
            slowestQuery=None,
            averageExecutionTime=0,
            commonPatterns=[],
            indexSuggestions=[],
            performanceIssues=[],
            recommendations=['æœªå‘ç°æ…¢æŸ¥è¯¢è®°å½•']
        )

    async def _generate_analysis_result(self, query_infos: List[SlowQueryInfo], raw_queries: List[Dict[str, Any]]) -> SlowQueryAnalysis:
        """ç”Ÿæˆå®Œæ•´çš„åˆ†æç»“æœ"""
        total_time = sum(q.executionTime for q in query_infos)
        avg_time = total_time / len(query_infos) if query_infos else 0

        # åˆ†ææŸ¥è¯¢æ¨¡å¼
        pattern_count: Dict[str, Dict[str, Union[int, float]]] = {}
        for query in query_infos:
            pattern = self._extract_query_pattern(query.sqlText)
            if pattern not in pattern_count:
                pattern_count[pattern] = {'count': 0, 'totalTime': 0.0}
            pattern_count[pattern]['count'] += 1
            pattern_count[pattern]['totalTime'] += query.executionTime

        common_patterns = [
            {
                'pattern': pattern,
                'count': stats['count'],
                'avgTime': stats['totalTime'] / stats['count']
            }
            for pattern, stats in pattern_count.items()
        ]
        common_patterns.sort(key=lambda x: x['count'], reverse=True)
        common_patterns = common_patterns[:10]

        # ç”Ÿæˆç´¢å¼•å»ºè®®
        index_suggestions = self._generate_index_suggestions(query_infos)

        # è¯†åˆ«æ€§èƒ½é—®é¢˜
        performance_issues = self._identify_performance_issues(query_infos, raw_queries)

        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        recommendations = self._generate_optimization_recommendations(len(query_infos), avg_time, common_patterns)

        return SlowQueryAnalysis(
            totalSlowQueries=len(query_infos),
            slowestQuery=query_infos[0] if query_infos else None,
            averageExecutionTime=avg_time,
            commonPatterns=common_patterns,
            indexSuggestions=index_suggestions,
            performanceIssues=performance_issues,
            recommendations=recommendations
        )

    def _extract_query_pattern(self, sql_text: str) -> str:
        """æå–æŸ¥è¯¢æ¨¡å¼å­—ç¬¦ä¸²"""
        import re
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œæ¨¡å¼æå–
        return re.sub(r'\s+', ' ', sql_text).upper().replace("'", '?').replace('"', '?')[:100]

    def _generate_index_suggestions(self, query_infos: List[SlowQueryInfo]) -> List[IndexSuggestion]:
        """ç”Ÿæˆç´¢å¼•ä¼˜åŒ–å»ºè®®"""
        suggestions = []

        for query in query_infos:
            if not query.usesIndex and query.executionTime > 1:
                upper_sql = query.sqlText.upper()

                if 'WHERE' in upper_sql and '=' in upper_sql:
                    table_match = re.search(r'FROM\s+(\w+)', query.sqlText, re.IGNORECASE)
                    table = table_match.group(1) if table_match else 'unknown_table'

                    # æ ¹æ®WHEREæ¡ä»¶ç”Ÿæˆä¸åŒçš„ç´¢å¼•å»ºè®®
                    columns = []
                    priority = Priority.MEDIUM
                    expected_improvement = '60-85%'

                    if 'WHERE ID =' in upper_sql or 'WHERE USER_ID =' in upper_sql:
                        columns = ['id']
                        priority = Priority.HIGH
                        expected_improvement = '70-90%'
                    elif 'WHERE EMAIL =' in upper_sql:
                        columns = ['email']
                        priority = Priority.MEDIUM
                    elif 'WHERE STATUS =' in upper_sql:
                        columns = ['status']
                        priority = Priority.MEDIUM

                    if columns:
                        suggestions.append(IndexSuggestion(
                            table=table,
                            columns=columns,
                            indexType=IndexType.INDEX,
                            expectedImprovement=expected_improvement,
                            priority=priority,
                            reason=f"WHEREå­å¥ä¸­é¢‘ç¹ä½¿ç”¨{', '.join(columns)}å­—æ®µè¿›è¡ŒæŸ¥è¯¢"
                        ))

                    # å¤åˆç´¢å¼•å»ºè®®
                    if 'WHERE' in upper_sql and (
                        upper_sql.count('AND') > 1 or
                        'ORDER BY' in upper_sql or
                        'GROUP BY' in upper_sql
                    ):
                        composite_columns = self._extract_composite_columns(upper_sql)
                        if len(composite_columns) > 1:
                            suggestions.append(IndexSuggestion(
                                table=table,
                                columns=composite_columns,
                                indexType=IndexType.INDEX,
                                expectedImprovement='70-95%',
                                priority=Priority.HIGH,
                                reason='å¤šæ¡ä»¶æŸ¥è¯¢ï¼Œå¤åˆç´¢å¼•å¯æ˜¾è‘—æå‡æ€§èƒ½'
                            ))

        return suggestions

    def _extract_composite_columns(self, upper_sql: str) -> List[str]:
        """æå–å¤åˆç´¢å¼•åˆ—"""
        columns = []
        where_clause = upper_sql.split('WHERE')[1].split('ORDER BY')[0].split('GROUP BY')[0] if 'WHERE' in upper_sql else ''

        field_regex = r'(\w+)\s*[=!><]'
        matches = re.findall(field_regex, where_clause)

        for field in matches:
            field_lower = field.lower()
            if field_lower not in ['and', 'or', 'not', 'is', 'null', 'exists', 'in']:
                if field_lower not in columns:
                    columns.append(field_lower)
                    if len(columns) >= 3:
                        break

        return columns

    def _identify_performance_issues(self, query_infos: List[SlowQueryInfo], raw_queries: List[Dict[str, Any]]) -> List[str]:
        """è¯†åˆ«æ€§èƒ½é—®é¢˜"""
        issues = []

        no_index_queries = len([q for q in query_infos if not q.usesIndex])
        if no_index_queries > len(query_infos) * 0.5:
            issues.append(f"å¤§é‡æŸ¥è¯¢æœªä½¿ç”¨ç´¢å¼• ({no_index_queries}/{len(query_infos)})")

        total_rows_examined = sum(q.rowsExamined for q in query_infos)
        avg_rows_examined = total_rows_examined / len(query_infos) if query_infos else 0

        if avg_rows_examined > 10000:
            issues.append(f"å¹³å‡æ‰«æè¡Œæ•°è¿‡é«˜ ({avg_rows_examined:.0f}è¡Œ)")

        long_running_queries = len([q for q in query_infos if q.executionTime > 5])
        if long_running_queries > 0:
            issues.append(f"å‘ç°{long_running_queries}ä¸ªæ‰§è¡Œæ—¶é—´è¶…è¿‡5ç§’çš„æŸ¥è¯¢")

        high_lock_time_queries = len([q for q in query_infos if q.lockTime > 1])
        if high_lock_time_queries > 0:
            issues.append(f"å‘ç°{high_lock_time_queries}ä¸ªé”ç­‰å¾…æ—¶é—´è¾ƒé•¿çš„æŸ¥è¯¢")

        return issues

    def _generate_optimization_recommendations(
        self,
        total_queries: int,
        avg_time: float,
        common_patterns: List[Dict[str, Any]]
    ) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        if avg_time > 2:
            recommendations.append('âš¡ æŸ¥è¯¢å¹³å‡æ‰§è¡Œæ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®åº“å‚æ•°è°ƒä¼˜')

        if len(common_patterns) > 3:
            recommendations.append('ğŸ”„ å‘ç°é‡å¤æŸ¥è¯¢æ¨¡å¼ï¼Œè€ƒè™‘ä½¿ç”¨æŸ¥è¯¢ç¼“å­˜æˆ–å­˜å‚¨è¿‡ç¨‹')

        if total_queries > 50:
            recommendations.append('ğŸ“Š æ…¢æŸ¥è¯¢æ•°é‡è¾ƒå¤šï¼Œå»ºè®®å¯ç”¨æ…¢æŸ¥è¯¢æ—¥å¿—è¿›è¡Œè¯¦ç»†åˆ†æ')

        if total_queries > 10:
            recommendations.append('ğŸ” å‘ç°å¤šä¸ªæ…¢æŸ¥è¯¢ï¼Œå»ºè®®æ·»åŠ é€‚å½“ç´¢å¼•')

        if not recommendations:
            recommendations.append('âœ… æŸ¥è¯¢æ€§èƒ½ç›¸å¯¹è‰¯å¥½ï¼Œå»ºè®®ç»§ç»­ç›‘æ§')

        return recommendations


class IndexOptimizationModule:
    """
    ç´¢å¼•ä¼˜åŒ–æ¨¡å—

    ä¸“é—¨ç”¨äºåˆ†ææ•°æ®åº“ç´¢å¼•ä½¿ç”¨æƒ…å†µå¹¶ç”Ÿæˆä¼˜åŒ–å»ºè®®çš„æ¨¡å—ã€‚é€šè¿‡åˆ†ææ…¢æŸ¥è¯¢æ¨¡å¼ã€
    ç°æœ‰ç´¢å¼•ä½¿ç”¨æƒ…å†µå’Œè¡¨ç»“æ„ï¼Œæä¾›é’ˆå¯¹æ€§çš„ç´¢å¼•åˆ›å»ºå»ºè®®ï¼Œä»¥æå‡æŸ¥è¯¢æ€§èƒ½ã€‚
    """

    def __init__(self, mysql_manager: MySQLManager, config: Optional[PerformanceAnalysisConfig] = None):
        """åˆå§‹åŒ–ç´¢å¼•ä¼˜åŒ–æ¨¡å—"""
        self.mysql_manager = mysql_manager
        self.config = config or PerformanceAnalysisConfig()

    async def generate_index_suggestions(self, limit: int = 50, time_range: str = '1 day') -> List[IndexSuggestion]:
        """ç”Ÿæˆç´¢å¼•ä¼˜åŒ–å»ºè®®"""
        try:
            # åˆå§‹åŒ–SlowQueryAnalysisModuleå®ä¾‹
            slow_query_module = SlowQueryAnalysisModule(self.mysql_manager, self.config)
            slow_query_analysis = await slow_query_module.analyze_slow_queries(limit, time_range)

            if slow_query_analysis.totalSlowQueries == 0:
                return await self._generate_general_index_recommendations()

            # åŸºäºæ…¢æŸ¥è¯¢æ•°æ®ç”Ÿæˆé’ˆå¯¹æ€§å»ºè®®
            suggestions = []
            analyzed_tables = set()

            for pattern in slow_query_analysis.commonPatterns:
                suggestions_from_pattern = await self._analyze_query_pattern(pattern['pattern'])
                for suggestion in suggestions_from_pattern:
                    if suggestion.table not in analyzed_tables:
                        suggestions.append(suggestion)
                        analyzed_tables.add(suggestion.table)

            # åˆ†æç°æœ‰ç´¢å¼•ä½¿ç”¨æƒ…å†µ
            existing_index_analysis = await self._analyze_existing_indexes()
            suggestions.extend(existing_index_analysis)

            return suggestions[:limit]
        except Exception as error:
            raise MySQLMCPError(
                f"ç´¢å¼•å»ºè®®ç”Ÿæˆå¤±è´¥: {str(error)}",
                ErrorCategory.DATA_ERROR,
                ErrorSeverity.MEDIUM
            )

    async def _analyze_query_pattern(self, pattern: str) -> List[IndexSuggestion]:
        """åˆ†ææŸ¥è¯¢æ¨¡å¼"""
        suggestions = []

        try:
            if 'FROM' in pattern and 'WHERE' in pattern:
                table_match = re.search(r'FROM\s+(\w+)', pattern, re.IGNORECASE)
                if table_match:
                    table_name = table_match.group(1)

                    # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                    if await self._check_table_exists(table_name):
                        # åˆ†æWHEREæ¡ä»¶ä¸­çš„å­—æ®µ
                        field_suggestions = self._extract_field_suggestions(pattern, table_name)
                        suggestions.extend(field_suggestions)
        except Exception:
            pass  # å¿½ç•¥åˆ†æé”™è¯¯ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªæ¨¡å¼

        return suggestions

    async def _check_table_exists(self, table_name: str) -> bool:
        """æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨"""
        try:
            query = f"SHOW TABLES LIKE '{table_name}'"
            result = await self.mysql_manager.execute_query(query)
            return len(result) > 0
        except Exception:
            return False

    def _extract_field_suggestions(self, pattern: str, table_name: str) -> List[IndexSuggestion]:
        """æå–å­—æ®µå»ºè®®"""
        suggestions = []
        upper_pattern = pattern.upper()

        # åˆ†æç›¸ç­‰æŸ¥è¯¢
        equal_conditions = re.findall(r'(\w+)\s*[=!]\s*[?\w]+', upper_pattern)
        for condition in equal_conditions[:5]:  # é™åˆ¶å»ºè®®æ•°é‡
            field_match = re.search(r'(\w+)\s*[=!]', condition)
            if field_match:
                field = field_match.group(1)
                if field.upper() not in ['AND', 'OR', 'NOT', 'IS', 'NULL', 'EXISTS', 'IN']:
                    suggestions.append(IndexSuggestion(
                        table=table_name,
                        columns=[field.lower()],
                        indexType=IndexType.INDEX,
                        expectedImprovement='50-80%',
                        priority=self._get_priority(field),
                        reason=f"WHEREæ¡ä»¶ä¸­é¢‘ç¹ä½¿ç”¨{field.lower()}å­—æ®µè¿›è¡Œç­‰å€¼æŸ¥è¯¢"
                    ))

        # åˆ†æèŒƒå›´æŸ¥è¯¢
        range_conditions = re.findall(r'(\w+)\s*[><]\s*=?\s*[?\w]+', upper_pattern)
        for condition in range_conditions[:3]:
            field_match = re.search(r'(\w+)\s*[><]', condition)
            if field_match:
                field = field_match.group(1)
                suggestions.append(IndexSuggestion(
                    table=table_name,
                    columns=[field.lower()],
                    indexType=IndexType.INDEX,
                    expectedImprovement='40-70%',
                    priority=Priority.MEDIUM,
                    reason=f"{field.lower()}å­—æ®µçš„èŒƒå›´æŸ¥è¯¢å¯é€šè¿‡ç´¢å¼•ä¼˜åŒ–"
                ))

        return suggestions

    def _get_priority(self, field: str) -> Priority:
        """è·å–å­—æ®µä¼˜å…ˆçº§"""
        high_priority_fields = ['id', 'user_id', 'created_at', 'updated_at']
        medium_priority_fields = ['email', 'status', 'category_id']

        field_lower = field.lower()
        if field_lower in high_priority_fields:
            return Priority.HIGH
        elif field_lower in medium_priority_fields:
            return Priority.MEDIUM
        return Priority.MEDIUM

    async def _analyze_existing_indexes(self) -> List[IndexSuggestion]:
        """åˆ†æç°æœ‰ç´¢å¼•"""
        suggestions = []

        try:
            # è·å–æ‰€æœ‰è¡¨
            tables_result = await self.mysql_manager.execute_query('SHOW TABLES')
            table_rows = [list(row.values())[0] for row in tables_result]

            for table_name in table_rows[:20]:  # é™åˆ¶åˆ†æçš„è¡¨æ•°é‡
                # æ£€æŸ¥è¡¨çš„ç´¢å¼•
                index_info = await self._get_table_index_info(table_name)
                suggestions_for_table = await self._analyze_table_index_health(table_name, index_info)
                suggestions.extend(suggestions_for_table)
        except Exception:
            pass  # å¿½ç•¥ç´¢å¼•åˆ†æé”™è¯¯

        return suggestions

    async def _get_table_index_info(self, table_name: str) -> List[Dict[str, Any]]:
        """è·å–è¡¨ç´¢å¼•ä¿¡æ¯"""
        query = """
            SELECT
                INDEX_NAME,
                COLUMN_NAME,
                SEQ_IN_INDEX,
                NON_UNIQUE,
                INDEX_TYPE
            FROM INFORMATION_SCHEMA.STATISTICS
            WHERE TABLE_SCHEMA = DATABASE()
                AND TABLE_NAME = ?
            ORDER BY INDEX_NAME, SEQ_IN_INDEX
        """

        return await self.mysql_manager.execute_query(query, [table_name])

    async def _analyze_table_index_health(self, table_name: str, indexes: List[Dict[str, Any]]) -> List[IndexSuggestion]:
        """åˆ†æè¡¨ç´¢å¼•å¥åº·çŠ¶å†µ"""
        suggestions = []

        # æ£€æŸ¥æ˜¯å¦ç¼ºå°‘ä¸»é”®
        has_primary_key = any(idx['INDEX_NAME'] == 'PRIMARY' for idx in indexes)
        if not has_primary_key:
            suggestions.append(IndexSuggestion(
                table=table_name,
                columns=['id'],  # å‡è®¾ä¸»é”®å­—æ®µåä¸ºid
                indexType=IndexType.PRIMARY,
                expectedImprovement='80-95%',
                priority=Priority.HIGH,
                reason='è¡¨ç¼ºå°‘ä¸»é”®ç´¢å¼•ï¼Œè¿™æ˜¯æ•°æ®åº“è®¾è®¡çš„æœ€ä½³å®è·µ'
            ))

        # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤ç´¢å¼•
        index_map = {}
        for idx in indexes:
            index_name = idx['INDEX_NAME']
            column_name = idx['COLUMN_NAME']
            if index_name not in index_map:
                index_map[index_name] = []
            index_map[index_name].append(column_name)

        for index_name, columns in index_map.items():
            for other_index_name, other_columns in index_map.items():
                if index_name != other_index_name and columns and other_columns:
                    if self._is_redundant_index(columns, other_columns):
                        suggestions.append(IndexSuggestion(
                            table=table_name,
                            columns=columns,
                            indexType=IndexType.INDEX,
                            expectedImprovement='5-10%',
                            priority=Priority.LOW,
                            reason=f"ç´¢å¼•{index_name}ä¸{other_index_name}å­˜åœ¨é‡å¤ï¼Œå¯è€ƒè™‘æ¸…ç†"
                        ))

        return suggestions

    def _is_redundant_index(self, columns1: List[str], columns2: List[str]) -> bool:
        """æ£€æŸ¥ç´¢å¼•æ˜¯å¦å†—ä½™"""
        if len(columns1) != len(columns2):
            return False
        return all(col in columns2 for col in columns1)

    async def _generate_general_index_recommendations(self) -> List[IndexSuggestion]:
        """ç”Ÿæˆé€šç”¨ç´¢å¼•å»ºè®®"""
        return [
            IndexSuggestion(
                table='general_recommendation',
                columns=['æ ‡å‡†å»ºè®®'],
                indexType=IndexType.INDEX,
                expectedImprovement='N/A',
                priority=Priority.MEDIUM,
                reason='å®šæœŸè¿è¡Œæ…¢æŸ¥è¯¢åˆ†æä»¥è·å–é’ˆå¯¹æ€§çš„ç´¢å¼•ä¼˜åŒ–å»ºè®®'
            ),
            IndexSuggestion(
                table='primary_key_check',
                columns=['ä¸»é”®æ£€æŸ¥'],
                indexType=IndexType.PRIMARY,
                expectedImprovement='é«˜',
                priority=Priority.HIGH,
                reason='ç¡®ä¿æ‰€æœ‰è¡¨éƒ½å®šä¹‰äº†åˆé€‚çš„ä¸»é”®ç´¢å¼•'
            )
        ]


class QueryProfilingModule:
    """
    æŸ¥è¯¢æ€§èƒ½å‰–ææ¨¡å—

    ä¸“é—¨ç”¨äºåˆ†æå•ä¸ªSQLæŸ¥è¯¢æ€§èƒ½çš„æ¨¡å—ï¼Œé€šè¿‡æ‰§è¡ŒEXPLAINå‘½ä»¤åˆ†ææŸ¥è¯¢æ‰§è¡Œè®¡åˆ’ï¼Œ
    æä¾›è¯¦ç»†çš„æ€§èƒ½åˆ†ææŠ¥å‘Šå’Œä¼˜åŒ–å»ºè®®ã€‚å¸®åŠ©è¯†åˆ«æŸ¥è¯¢ä¸­çš„æ€§èƒ½ç“¶é¢ˆå’Œæ”¹è¿›æœºä¼šã€‚
    """

    def __init__(self, mysql_manager: MySQLManager):
        """åˆå§‹åŒ–æŸ¥è¯¢æ€§èƒ½å‰–ææ¨¡å—"""
        self.mysql_manager = mysql_manager

    async def profile_query(self, sql: str, params: Optional[List[Dict[str, Any]]] = None) -> QueryProfileResult:
        """å¯¹ç‰¹å®šæŸ¥è¯¢è¿›è¡Œæ€§èƒ½å‰–æ"""
        try:
            # éªŒè¯æŸ¥è¯¢
            await self.mysql_manager.validate_input(sql, 'query')

            # æ‰§è¡ŒEXPLAINåˆ†æ
            explain_json = await self._get_explain_result(sql, params)
            explain_simple = await self._get_simple_explain_result(sql, params)

            # è·å–æ‰§è¡Œç»Ÿè®¡ï¼ˆå¦‚æœå¯èƒ½ï¼‰
            execution_stats = await self._get_execution_stats(sql, params)

            # åˆ†ææ‰§è¡Œè®¡åˆ’å¹¶ç”Ÿæˆå»ºè®®
            recommendations = self._analyze_explain_result(explain_json, explain_simple)
            performance_score = self._calculate_performance_score(explain_json, execution_stats)

            return QueryProfileResult(
                explainResult=explain_json,
                executionStats=execution_stats,
                recommendations=recommendations,
                performanceScore=performance_score
            )
        except Exception as error:
            raise MySQLMCPError(
                f"æŸ¥è¯¢æ€§èƒ½å‰–æå¤±è´¥: {str(error)}",
                ErrorCategory.UNKNOWN,
                ErrorSeverity.MEDIUM
            )

    async def _get_explain_result(self, sql: str, params: Optional[List[Dict[str, Any]]] = None) -> List[Dict[str, Any]]:
        """è·å–EXPLAINç»“æœï¼ˆJSONæ ¼å¼ï¼‰"""
        try:
            explain_query = f"EXPLAIN FORMAT=JSON {sql}"
            result = await self.mysql_manager.execute_query(explain_query, params)
            return result if isinstance(result, list) else [result] if result else []
        except Exception:
            # å¦‚æœFORMAT=JSONä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæ ¼å¼
            return await self._get_simple_explain_result(sql, params)

    async def _get_simple_explain_result(self, sql: str, params: Optional[List[Dict[str, Any]]] = None) -> List[Dict[str, Any]]:
        """è·å–æ ‡å‡†EXPLAINç»“æœ"""
        explain_query = f"EXPLAIN {sql}"
        result = await self.mysql_manager.execute_query(explain_query, params)
        return result if isinstance(result, list) else [result] if result else []

    async def _get_execution_stats(self, sql: str, params: Optional[List[Dict[str, Any]]] = None) -> Dict[str, Union[int, float]]:
        """è·å–æ‰§è¡Œç»Ÿè®¡ä¿¡æ¯"""
        try:
            # å¯ç”¨æŸ¥è¯¢æ€§èƒ½åˆ†æ
            await self.mysql_manager.execute_query('SET profiling = 1')

            # æ‰§è¡ŒæŸ¥è¯¢
            await self.mysql_manager.execute_query(sql, params)

            # è·å–æ€§èƒ½åˆ†æä¿¡æ¯
            profile_result = await self.mysql_manager.execute_query('SHOW PROFILE')
            total_duration = sum(row.get('Duration', 0) for row in profile_result)

            return {
                'executionTime': total_duration * 1000,  # è½¬æ¢ä¸ºæ¯«ç§’
                'rowsExamined': -1,  # EXPLAINä¸­è·å–æ›´å‡†ç¡®çš„ä¿¡æ¯
                'rowsReturned': -1  # EXPLAINä¸­è·å–æ›´å‡†ç¡®çš„ä¿¡æ¯
            }
        except Exception:
            return {
                'executionTime': -1,
                'rowsExamined': -1,
                'rowsReturned': -1
            }

    def _analyze_explain_result(self, explain_json: List[Dict[str, Any]], explain_simple: List[Dict[str, Any]]) -> List[str]:
        """åˆ†æEXPLAINç»“æœå¹¶ç”Ÿæˆå»ºè®®"""
        recommendations = []

        try:
            # åˆ†æç®€å•çš„EXPLAINç»“æœ
            for index, row in enumerate(explain_simple):
                # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†å…¨è¡¨æ‰«æ
                if row.get('type') == 'ALL':
                    recommendations.append(f"æŸ¥è¯¢æ­¥éª¤{index + 1}ï¼šä½¿ç”¨å…¨è¡¨æ‰«æï¼Œå»ºè®®æ·»åŠ ç´¢å¼•")

                # æ£€æŸ¥ç´¢å¼•ä½¿ç”¨æƒ…å†µ
                if not row.get('key'):
                    recommendations.append(f"æŸ¥è¯¢æ­¥éª¤{index + 1}ï¼šæœªä½¿ç”¨ç´¢å¼•ï¼ŒæŸ¥è¯¢æ€§èƒ½å¯èƒ½è¾ƒå·®")

                # æ£€æŸ¥æ‰«æè¡Œæ•°
                rows = row.get('rows')
                if rows and isinstance(rows, (int, float)) and rows > 1000:
                    recommendations.append(f"æŸ¥è¯¢æ­¥éª¤{index + 1}ï¼šæ‰«æ{rows}è¡Œæ•°æ®ï¼Œå»ºè®®ä¼˜åŒ–ç´¢å¼•æˆ–æŸ¥è¯¢æ¡ä»¶")

                # æ£€æŸ¥Extraå­—æ®µçš„å…³é”®ä¿¡æ¯
                extra = row.get('Extra', '')
                if extra:
                    if 'Using temporary' in extra:
                        recommendations.append(f"æŸ¥è¯¢æ­¥éª¤{index + 1}ï¼šä½¿ç”¨ä¸´æ—¶è¡¨ï¼Œå»ºè®®ä¼˜åŒ–GROUP BYæˆ–ORDER BY")
                    if 'Using filesort' in extra:
                        recommendations.append(f"æŸ¥è¯¢æ­¥éª¤{index + 1}ï¼šä½¿ç”¨æ–‡ä»¶æ’åºï¼Œå»ºè®®ä¼˜åŒ–ORDER BYç´¢å¼•")
                    if 'Using where' in extra:
                        recommendations.append(f"æŸ¥è¯¢æ­¥éª¤{index + 1}ï¼šä½¿ç”¨WHEREæ¡ä»¶è¿‡æ»¤ï¼Œç´¢å¼•æ¨èæœ‰æ•ˆ")
                    if 'Using index' in extra:
                        recommendations.append(f"æŸ¥è¯¢æ­¥éª¤{index + 1}ï¼šä½¿ç”¨è¦†ç›–ç´¢å¼•ï¼ŒæŸ¥è¯¢æ€§èƒ½è‰¯å¥½")

                # æ£€æŸ¥possible_keyså­—æ®µ
                possible_keys = row.get('possible_keys')
                if not possible_keys:
                    recommendations.append(f"æŸ¥è¯¢æ­¥éª¤{index + 1}ï¼šæ²¡æœ‰å¯ç”¨çš„ç´¢å¼•ï¼Œå»ºè®®ä¸ºæŸ¥è¯¢æ¡ä»¶æ·»åŠ ç´¢å¼•")

            # å¦‚æœæ²¡æœ‰å…·ä½“çš„å»ºè®®ï¼Œæä¾›é€šç”¨å»ºè®®
            if not recommendations:
                recommendations.append('æŸ¥è¯¢æ‰§è¡Œè®¡åˆ’æ­£å¸¸ï¼Œå»ºè®®ç»§ç»­ç›‘æ§æ€§èƒ½è¡¨ç°')

            # æ·»åŠ æ ‡å‡†çš„ä¼˜åŒ–å»ºè®®
            full_table_scans = [
                row for row in explain_simple
                if row.get('type') == 'ALL' and isinstance(row.get('rows'), (int, float)) and row.get('rows', 0) > 1000
            ]

            if full_table_scans:
                recommendations.append('è€ƒè™‘ä¸ºç›¸å…³å­—æ®µæ·»åŠ ç´¢å¼•ä»¥å‡å°‘å…¨è¡¨æ‰«æ')

            if len(explain_simple) > 3:
                recommendations.append('æŸ¥è¯¢æ¶‰åŠå¤šä¸ªè¡¨ï¼Œå»ºè®®æ£€æŸ¥JOINæ¡ä»¶å’Œç´¢å¼•')

            # åˆ†æJSONæ ¼å¼çš„EXPLAINç»“æœï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if explain_json:
                json_analysis = self._analyze_json_explain(explain_json)
                recommendations.extend(json_analysis)

        except Exception as error:
            recommendations.append(f"åˆ†æè§£é‡Šç»“æœæ—¶å‡ºç°é”™è¯¯: {str(error)}")

        return recommendations

    def _analyze_json_explain(self, explain_json: List[Dict[str, Any]]) -> List[str]:
        """åˆ†æJSONæ ¼å¼çš„EXPLAINç»“æœ"""
        recommendations = []

        try:
            if not explain_json:
                return recommendations

            # JSONæ ¼å¼çš„EXPLAINé€šå¸¸åœ¨ç¬¬ä¸€ä¸ªå…ƒç´ çš„EXPLAINå­—æ®µä¸­åŒ…å«è¯¦ç»†ä¿¡æ¯
            explain_data = explain_json[0]

            if isinstance(explain_data, dict):
                # æ£€æŸ¥æ˜¯å¦æœ‰EXPLAINå­—æ®µï¼ˆMySQL 5.7+çš„JSONæ ¼å¼ï¼‰
                explain_field = explain_data.get('EXPLAIN')
                if explain_field:
                    json_plan = explain_field if isinstance(explain_field, dict) else explain_field

                    # åˆ†ææŸ¥è¯¢å—
                    if isinstance(json_plan, dict) and 'query_block' in json_plan:
                        query_block = json_plan['query_block']

                        # æ£€æŸ¥æˆæœ¬
                        if 'cost_info' in query_block:
                            cost = float(query_block['cost_info'].get('query_cost', '0'))
                            if cost > 1000:
                                recommendations.append(f"æŸ¥è¯¢æˆæœ¬è¾ƒé«˜ ({cost:.2f})ï¼Œå»ºè®®ä¼˜åŒ–")

                        # åˆ†æåµŒå¥—å¾ªç¯
                        if 'nested_loop' in query_block:
                            tables = query_block['nested_loop']
                            if isinstance(tables, list) and len(tables) > 3:
                                recommendations.append('æŸ¥è¯¢æ¶‰åŠå¤šä¸ªåµŒå¥—å¾ªç¯ï¼Œè€ƒè™‘ä¼˜åŒ–JOINç­–ç•¥')

                        # åˆ†æè¡¨è®¿é—®
                        if 'table' in query_block:
                            table = query_block['table']
                            if table.get('access_type') == 'ALL':
                                recommendations.append(f"è¡¨ {table.get('table_name')} ä½¿ç”¨å…¨è¡¨æ‰«æï¼Œå»ºè®®æ·»åŠ ç´¢å¼•")

                            # æ£€æŸ¥è¿‡æ»¤æ•ˆç‡
                            if 'filtered' in table:
                                filtered = float(table['filtered'])
                                if filtered < 50:
                                    recommendations.append(f"è¡¨ {table.get('table_name')} çš„è¿‡æ»¤æ•ˆç‡ä½ ({filtered}%)ï¼Œå»ºè®®ä¼˜åŒ–æ¡ä»¶")

                        # åˆ†ææ’åºæ“ä½œ
                        if 'ordering_operation' in query_block:
                            if query_block['ordering_operation'].get('using_filesort'):
                                recommendations.append('æŸ¥è¯¢ä½¿ç”¨æ–‡ä»¶æ’åºï¼Œå»ºè®®ä¼˜åŒ–ORDER BYç´¢å¼•')

                        # åˆ†æåˆ†ç»„æ“ä½œ
                        if 'grouping_operation' in query_block:
                            if query_block['grouping_operation'].get('using_temporary_table'):
                                recommendations.append('æŸ¥è¯¢ä½¿ç”¨ä¸´æ—¶è¡¨è¿›è¡Œåˆ†ç»„ï¼Œå»ºè®®ä¼˜åŒ–GROUP BY')

        except Exception as error:
            logger.warn(f"åˆ†æJSON EXPLAINç»“æœæ—¶å‡ºé”™: {str(error)}")

        return recommendations

    def _calculate_performance_score(self, explain_json: List[Dict[str, Any]], execution_stats: Dict[str, Union[int, float]]) -> float:
        """è®¡ç®—æ€§èƒ½è¯„åˆ†"""
        try:
            score = 100.0

            # å¦‚æœæ— æ³•è·å–æ‰§è¡Œç»Ÿè®¡ï¼Œä½¿ç”¨æ‰§è¡Œè®¡åˆ’ä¼°ç®—
            if execution_stats.get('executionTime', -1) == -1:
                # åŸºäºæ‰§è¡Œè®¡åˆ’ä¼°ç®—åˆ†æ•°
                for row in explain_json:
                    # å…¨è¡¨æ‰«æä¸¥é‡å‡åˆ†
                    if row.get('table') and row.get('access_type') == 'ALL':
                        score -= 30
                    # å¤§é‡æ‰«æè¡Œæ•°å‡åˆ†
                    if row.get('rows') and isinstance(row.get('rows'), (int, float)) and row.get('rows', 0) > 10000:
                        score -= 20
                    # ç´¢å¼•æ‰«ææƒ…å†µ
                    if row.get('key'):
                        score += 10
            else:
                # åŸºäºå®é™…æ‰§è¡Œæ—¶é—´æ‰“åˆ†
                exec_time = execution_stats['executionTime']
                if exec_time > 5000:
                    score -= 50  # 5ç§’ä»¥ä¸Šä¸¥é‡å‡åˆ†
                elif exec_time > 1000:
                    score -= 30  # 1ç§’ä»¥ä¸Šå‡åˆ†
                elif exec_time > 500:
                    score -= 15  # 500msä»¥ä¸Šå°å¹…å‡åˆ†
                elif exec_time < 50:
                    score += 10  # å¿«æŸ¥è¯¢åŠ åˆ†

            return max(0.0, min(100.0, score))
        except Exception:
            return 50.0  # æ— æ³•åˆ†ææ—¶è¿”å›ä¸­ç­‰åˆ†æ•°


class PerformanceMonitoringModule:
    """
    æ€§èƒ½ç›‘æ§æ¨¡å—

    ç”¨äºæŒç»­ç›‘æ§MySQLæ•°æ®åº“æ€§èƒ½çš„æ¨¡å—ï¼Œå®šæœŸåˆ†ææ…¢æŸ¥è¯¢å¹¶æä¾›å®æ—¶æ€§èƒ½åé¦ˆã€‚
    æ”¯æŒé…ç½®ç›‘æ§é—´éš”å’Œè‡ªåŠ¨å‘Šè­¦åŠŸèƒ½ï¼Œå¸®åŠ©åŠæ—¶å‘ç°å’Œè§£å†³æ€§èƒ½é—®é¢˜ã€‚
    """

    def __init__(self, mysql_manager: MySQLManager, config: Optional[PerformanceAnalysisConfig] = None):
        """åˆå§‹åŒ–æ€§èƒ½ç›‘æ§æ¨¡å—"""
        self.mysql_manager = mysql_manager
        self.slow_query_analysis = SlowQueryAnalysisModule(mysql_manager, config)
        self.monitoring_active = False
        self.monitoring_interval = None

    async def start_monitoring(self, config: Optional[SlowQueryConfig] = None, interval_minutes: int = 60) -> None:
        """å¯åŠ¨æ€§èƒ½ç›‘æ§"""
        try:
            # å¯ç”¨æ…¢æŸ¥è¯¢æ—¥å¿—
            await self.slow_query_analysis.enable_slow_query_log(config)

            self.monitoring_active = True

            # è®¾ç½®å®šæœŸç›‘æ§
            self.monitoring_interval = asyncio.get_event_loop().call_later(
                interval_minutes * 60,
                self._monitoring_task,
                interval_minutes
            )

            logger.warn(f"ğŸ” [æ€§èƒ½ç›‘æ§] å¼€å§‹ç›‘æ§ï¼Œæ¯ {interval_minutes} åˆ†é’Ÿåˆ†æä¸€æ¬¡")
        except Exception as error:
            raise MySQLMCPError(
                f"å¯åŠ¨æ€§èƒ½ç›‘æ§å¤±è´¥: {str(error)}",
                ErrorCategory.CONFIGURATION_ERROR,
                ErrorSeverity.HIGH
            )

    def stop_monitoring(self) -> None:
        """åœæ­¢æ€§èƒ½ç›‘æ§"""
        if self.monitoring_interval:
            self.monitoring_interval.cancel()
            self.monitoring_interval = None
        self.monitoring_active = False
        logger.warn("â¹ï¸ [æ€§èƒ½ç›‘æ§] æ€§èƒ½ç›‘æ§å·²åœæ­¢")

    def get_monitoring_status(self) -> Dict[str, Any]:
        """è·å–ç›‘æ§çŠ¶æ€"""
        return {
            'active': self.monitoring_active,
            'config': self.slow_query_analysis.config.__dict__ if self.slow_query_analysis.config else {}
        }

    async def _monitoring_task(self, interval_minutes: int) -> None:
        """ç›‘æ§ä»»åŠ¡"""
        try:
            analysis = await self.slow_query_analysis.analyze_slow_queries(20, '1 hour')

            if analysis.totalSlowQueries > 0:
                logger.warn(f"âš ï¸ [æ€§èƒ½ç›‘æ§] æ£€æµ‹åˆ° {analysis.totalSlowQueries} ä¸ªæ…¢æŸ¥è¯¢")
                logger.warn(f"ğŸ“Š [æ€§èƒ½ç›‘æ§] æœ€æ…¢æŸ¥è¯¢è€—æ—¶: {analysis.slowestQuery.executionTime:.2f}s" if analysis.slowestQuery else "")

                if analysis.indexSuggestions:
                    logger.warn(f"ğŸ’¡ [æ€§èƒ½ç›‘æ§] å‘ç° {len(analysis.indexSuggestions)} ä¸ªç´¢å¼•ä¼˜åŒ–å»ºè®®")
            else:
                logger.warn("âœ… [æ€§èƒ½ç›‘æ§] æŸ¥è¯¢æ€§èƒ½æ­£å¸¸")

            # ç»§ç»­ä¸‹ä¸€ä¸ªç›‘æ§å‘¨æœŸ
            if self.monitoring_active:
                self.monitoring_interval = asyncio.get_event_loop().call_later(
                    interval_minutes * 60,
                    self._monitoring_task,
                    interval_minutes
                )
        except Exception as error:
            logger.error(f"âŒ [æ€§èƒ½ç›‘æ§] ç›‘æ§è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {str(error)}")


class ReportingModule:
    """
    æŠ¥å‘Šç”Ÿæˆæ¨¡å—

    ä¸“é—¨ç”¨äºç”Ÿæˆç»¼åˆMySQLæ•°æ®åº“æ€§èƒ½æŠ¥å‘Šçš„æ¨¡å—ï¼Œæ•´åˆæ…¢æŸ¥è¯¢åˆ†æã€ç³»ç»ŸçŠ¶æ€ç›‘æµ‹å’Œä¼˜åŒ–å»ºè®®ã€‚
    é€šè¿‡å¤šç»´åº¦æ•°æ®æ”¶é›†å’Œæ·±åº¦åˆ†æï¼Œç”Ÿæˆç»“æ„åŒ–çš„æ€§èƒ½æŠ¥å‘Šå¸®åŠ©æ•°æ®åº“ç®¡ç†å‘˜è¿›è¡Œæ€§èƒ½è¯Šæ–­
    å’Œä¼˜åŒ–å†³ç­–ã€‚æ”¯æŒçµæ´»çš„æŠ¥å‘Šé…ç½®å’Œè¯¦ç»†ç¨‹åº¦æ§åˆ¶ï¼Œæ»¡è¶³ä¸åŒåœºæ™¯çš„æŠ¥å‘Šéœ€æ±‚ã€‚
    """

    def __init__(self, mysql_manager: MySQLManager, config: Optional[PerformanceAnalysisConfig] = None):
        """åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆæ¨¡å—"""
        self.mysql_manager = mysql_manager
        self.slow_query_analysis = SlowQueryAnalysisModule(mysql_manager, config)

    async def generate_report(self, limit: int = 50, time_range: str = '1 day', include_details: bool = True) -> PerformanceReport:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        try:
            # è·å–æ…¢æŸ¥è¯¢åˆ†æ
            slow_query_analysis = await self.slow_query_analysis.analyze_slow_queries(limit, time_range)

            # è·å–ç³»ç»ŸçŠ¶æ€
            system_status = await self._get_system_status()

            # ç”Ÿæˆä¼˜åŒ–å»ºè®®
            recommendations = self._generate_comprehensive_recommendations(slow_query_analysis, system_status)

            report = PerformanceReport(
                generatedAt=datetime.now(),
                summary={
                    'slowQueriesCount': slow_query_analysis.totalSlowQueries,
                    'averageExecutionTime': slow_query_analysis.averageExecutionTime,
                    'recommendationsCount': len(recommendations)
                },
                slowQueryAnalysis=slow_query_analysis,
                systemStatus=system_status,
                recommendations=recommendations
            )

            return report
        except Exception as error:
            raise MySQLMCPError(
                f"ç”Ÿæˆæ€§èƒ½æŠ¥å‘Šå¤±è´¥: {str(error)}",
                ErrorCategory.UNKNOWN,
                ErrorSeverity.MEDIUM
            )

    async def _get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€ä¿¡æ¯"""
        try:
            # è·å–è¿æ¥ä¿¡æ¯
            connection_query = "SELECT COUNT(*) as active_connections FROM information_schema.processlist WHERE COMMAND != 'Sleep'"
            connection_result = await self.mysql_manager.execute_query(connection_query)
            active_connections = connection_result[0]['active_connections'] if connection_result else 0

            # è·å–ç‰ˆæœ¬ä¿¡æ¯
            version_query = "SELECT VERSION() as mysql_version"
            version_result = await self.mysql_manager.execute_query(version_query)

            return {
                'connectionHealth': 'healthy' if active_connections < 50 else 'warning' if active_connections < 100 else 'critical',
                'memoryUsage': 'é€šè¿‡ç³»ç»Ÿç›‘æ§è·å–',
                'activeConnections': active_connections,
                'system': {
                    'mysql_version': version_result[0]['mysql_version'] if version_result else 'unknown',
                    'query_cache_hit_rate': await self._get_query_cache_hit_rate(),
                    'innodb_buffer_pool_hit_rate': await self._get_innodb_buffer_pool_hit_rate()
                }
            }
        except Exception:
            return {
                'connectionHealth': 'unknown',
                'memoryUsage': 'unknown',
                'activeConnections': -1,
                'error': 'ç³»ç»ŸçŠ¶æ€è·å–å¤±è´¥'
            }

    async def _get_query_cache_hit_rate(self) -> str:
        """è·å–æŸ¥è¯¢ç¼“å­˜å‘½ä¸­ç‡"""
        try:
            # æŸ¥è¯¢ç¼“å­˜ç›¸å…³çš„MySQLçŠ¶æ€å˜é‡
            cache_status_query = """
                SHOW GLOBAL STATUS WHERE Variable_name IN (
                    'Qcache_queries_in_cache',
                    'Qcache_hits',
                    'Qcache_inserts',
                    'Qcache_not_cached',
                    'Qcache_lowmem_prunes'
                )
            """
            cache_result = await self.mysql_manager.execute_query(cache_status_query)

            if not cache_result:
                return '0.0%'  # æŸ¥è¯¢ç¼“å­˜ä¸å¯ç”¨

            # å°†çŠ¶æ€ç»“æœè½¬æ¢ä¸ºå¯¹è±¡
            cache_stats = {}
            for row in cache_result:
                cache_stats[row['Variable_name']] = int(row['Value']) if row['Value'] else 0

            hits = cache_stats.get('Qcache_hits', 0)
            inserts = cache_stats.get('Qcache_inserts', 0)
            not_cached = cache_stats.get('Qcache_not_cached', 0)

            # è®¡ç®—æ€»æŸ¥è¯¢æ•°å’Œå‘½ä¸­ç‡
            total_queries = hits + inserts + not_cached
            if total_queries == 0:
                return '0.0%'

            hit_rate = (hits / total_queries) * 100
            return f"{hit_rate:.1f}%"
        except Exception as error:
            # æŸ¥è¯¢ç¼“å­˜å¯èƒ½ä¸å¯ç”¨æˆ–è¢«ç¦ç”¨
            logger.warn(f"è·å–æŸ¥è¯¢ç¼“å­˜å‘½ä¸­ç‡å¤±è´¥: {str(error)}")
            return 'N/A'

    async def _get_innodb_buffer_pool_hit_rate(self) -> str:
        """è·å–InnoDBç¼“å†²æ± å‘½ä¸­ç‡"""
        try:
            # æŸ¥è¯¢InnoDBç›¸å…³çš„MySQLçŠ¶æ€å˜é‡
            innodb_status_query = """
                SHOW GLOBAL STATUS WHERE Variable_name IN (
                    'Innodb_buffer_pool_reads',
                    'Innodb_buffer_pool_read_requests'
                )
            """
            innodb_result = await self.mysql_manager.execute_query(innodb_status_query)

            if not innodb_result:
                return 'N/A'  # InnoDBç»Ÿè®¡ä¸å¯ç”¨

            # å°†çŠ¶æ€ç»“æœè½¬æ¢ä¸ºå¯¹è±¡
            innodb_stats = {}
            for row in innodb_result:
                innodb_stats[row['Variable_name']] = int(row['Value']) if row['Value'] else 0

            buffer_reads = innodb_stats.get('Innodb_buffer_pool_reads', 0)
            read_requests = innodb_stats.get('Innodb_buffer_pool_read_requests', 0)

            # è®¡ç®—ç¼“å†²æ± å‘½ä¸­ç‡
            if read_requests == 0:
                return '100.0%'  # æ— è¯»å–è¯·æ±‚ï¼Œè®¤ä¸ºæ˜¯100%å‘½ä¸­

            hit_rate = ((read_requests - buffer_reads) / read_requests) * 100
            return f"{max(0, hit_rate):.1f}%"
        except Exception as error:
            # InnoDBç»Ÿè®¡å¯èƒ½ä¸å¯ç”¨
            logger.warn(f"è·å–InnoDBç¼“å†²æ± å‘½ä¸­ç‡å¤±è´¥: {str(error)}")
            return 'N/A'

    def _generate_comprehensive_recommendations(
        self,
        analysis: SlowQueryAnalysis,
        system_status: Dict[str, Any]
    ) -> List[str]:
        """ç”Ÿæˆç»¼åˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # åŸºäºæ…¢æŸ¥è¯¢åˆ†æçš„å»ºè®®
        if analysis.recommendations:
            recommendations.extend(analysis.recommendations)

        # ç³»ç»Ÿçº§å»ºè®®
        connection_health = system_status.get('connectionHealth', 'unknown')
        if connection_health == 'critical':
            recommendations.append('ğŸ”— è¿æ¥æ•°è¿‡é«˜ï¼Œå»ºè®®å¢åŠ è¿æ¥æ± å¤§å°æˆ–ä¼˜åŒ–æŸ¥è¯¢æ•ˆç‡')

        if analysis.totalSlowQueries > 100:
            recommendations.append('ğŸ“Š å¤§é‡æ…¢æŸ¥è¯¢å‘ç°ï¼Œå»ºè®®å¯ç”¨æŸ¥è¯¢ç¼“å­˜æˆ–è¿›è¡Œå…¨é¢çš„ç´¢å¼•ä¼˜åŒ–')

        if analysis.averageExecutionTime > 5:
            recommendations.append('âš¡ å¹³å‡æŸ¥è¯¢æ‰§è¡Œæ—¶é—´è¿‡é•¿ï¼Œå»ºè®®è¿›è¡ŒæœåŠ¡å™¨å‚æ•°è°ƒä¼˜')

        if not recommendations:
            recommendations.append('âœ… ç³»ç»Ÿæ€§èƒ½è‰¯å¥½ï¼Œç»§ç»­ä¿æŒå½“å‰çš„ä¼˜åŒ–æªæ–½')

        return recommendations


class PerformanceManager:
    """
    ç»Ÿä¸€æ€§èƒ½ç®¡ç†å™¨ç±» - åˆå¹¶æ‰€æœ‰æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½

    ä¼ä¸šçº§MySQLæ€§èƒ½ç®¡ç†çš„æ ¸å¿ƒç»„ä»¶ï¼Œæ•´åˆäº†æ…¢æŸ¥è¯¢åˆ†æã€ç´¢å¼•ä¼˜åŒ–ã€æŸ¥è¯¢å‰–æã€
    æ€§èƒ½ç›‘æ§å’ŒæŠ¥å‘Šç”Ÿæˆç­‰äº”å¤§æ ¸å¿ƒåŠŸèƒ½æ¨¡å—ã€‚æä¾›ç»Ÿä¸€çš„æ€§èƒ½ä¼˜åŒ–å…¥å£å’Œé…ç½®ç®¡ç†ï¼Œ
    æ”¯æŒå¤šç§æ€§èƒ½ä¼˜åŒ–æ“ä½œçš„é›†ä¸­è°ƒåº¦å’Œæ‰§è¡Œã€‚
    """

    def __init__(self, mysql_manager: MySQLManager, config: Optional[PerformanceAnalysisConfig] = None):
        """åˆå§‹åŒ–æ€§èƒ½ç®¡ç†å™¨"""
        self.mysql_manager = mysql_manager
        self.config = PerformanceAnalysisConfig(
            longQueryTime=1,
            timeRange=1,
            includeDetails=True,
            limit=100,
            minExaminedRowLimit=1000,
            enablePerformanceSchema=True,
            logQueriesNotUsingIndexes=True,
            maxLogFileSize=100,
            logSlowAdminStatements=True,
            **(config.__dict__ if config else {})
        )

        # åˆå§‹åŒ–å­æ¨¡å—
        self.slowQueryAnalysis = SlowQueryAnalysisModule(mysql_manager, self.config)
        self.indexOptimization = IndexOptimizationModule(mysql_manager, self.config)
        self.queryProfiling = QueryProfilingModule(mysql_manager)
        self.performanceMonitoring = PerformanceMonitoringModule(mysql_manager, self.config)
        self.reporting = ReportingModule(mysql_manager, self.config)

    async def configure_slow_query_log(self, long_query_time: float = 1) -> None:
        """é…ç½®æ…¢æŸ¥è¯¢æ—¥å¿—"""
        try:
            settings = [
                'SET GLOBAL slow_query_log = "ON"',
                f'SET GLOBAL long_query_time = {long_query_time}',
                'SET GLOBAL log_queries_not_using_indexes = "ON"',
                'SET GLOBAL log_slow_admin_statements = "ON"'
            ]

            for setting in settings:
                await self.mysql_manager.execute_query(setting)

            logger.warn(f"âœ… æ…¢æŸ¥è¯¢æ—¥å¿—å·²é…ç½®ï¼Œé˜ˆå€¼: {long_query_time}ç§’")
        except Exception as error:
            raise MySQLMCPError(
                f"é…ç½®æ…¢æŸ¥è¯¢æ—¥å¿—å¤±è´¥: {str(error)}",
                ErrorCategory.CONFIGURATION_ERROR,
                ErrorSeverity.HIGH
            )

    async def get_slow_query_log_config(self) -> Dict[str, Any]:
        """è·å–æ…¢æŸ¥è¯¢æ—¥å¿—é…ç½®"""
        try:
            queries = [
                'SELECT @@slow_query_log as enabled',
                'SELECT @@long_query_time as threshold',
                'SELECT @@slow_query_log_file as log_file',
                'SELECT @@log_queries_not_using_indexes as log_no_index'
            ]

            config = {}

            for query in queries:
                result = await self.mysql_manager.execute_query(query)
                if result:
                    config.update(result[0])

            return config
        except Exception as error:
            raise MySQLMCPError(
                f"è·å–æ…¢æŸ¥è¯¢æ—¥å¿—é…ç½®å¤±è´¥: {str(error)}",
                ErrorCategory.DATA_ERROR,
                ErrorSeverity.LOW
            )

    async def disable_slow_query_log(self) -> None:
        """ç¦ç”¨æ…¢æŸ¥è¯¢æ—¥å¿—"""
        try:
            await self.mysql_manager.execute_query('SET GLOBAL slow_query_log = "OFF"')
            logger.warn("â¹ï¸ æ…¢æŸ¥è¯¢æ—¥å¿—å·²ç¦ç”¨")
        except Exception as error:
            raise MySQLMCPError(
                f"ç¦ç”¨æ…¢æŸ¥è¯¢æ—¥å¿—å¤±è´¥: {str(error)}",
                ErrorCategory.CONFIGURATION_ERROR,
                ErrorSeverity.LOW
            )

    async def optimize_performance(
        self,
        action: str,
        options: Optional[Dict[str, Any]] = None
    ) -> Any:
        """ç»Ÿä¸€æ€§èƒ½ä¼˜åŒ–å…¥å£æ–¹æ³•"""
        try:
            options = options or {}

            if action == 'analyze_slow_queries':
                return await self.slowQueryAnalysis.analyze_slow_queries(
                    options.get('limit', 100),
                    options.get('timeRange', '1 day')
                )

            elif action == 'suggest_indexes':
                return await self.indexOptimization.generate_index_suggestions(
                    options.get('limit', 50),
                    options.get('timeRange', '1 day')
                )

            elif action == 'performance_report':
                return await self.reporting.generate_report(
                    options.get('limit', 50),
                    options.get('timeRange', '1 day'),
                    options.get('includeDetails', True)
                )

            elif action == 'query_profiling':
                if not options.get('query'):
                    raise MySQLMCPError(
                        'query_profilingæ“ä½œå¿…é¡»æä¾›queryå‚æ•°',
                        ErrorCategory.INVALID_INPUT,
                        ErrorSeverity.MEDIUM
                    )
                return await self.queryProfiling.profile_query(
                    options['query'],
                    options.get('params')
                )

            elif action == 'start_monitoring':
                await self.performanceMonitoring.start_monitoring(
                    SlowQueryConfig(
                        longQueryTime=options.get('longQueryTime'),
                        logQueriesNotUsingIndexes=options.get('logQueriesNotUsingIndexes')
                    ),
                    options.get('monitoringIntervalMinutes', 60)
                )
                return {'message': 'æ€§èƒ½ç›‘æ§å·²å¯åŠ¨'}

            elif action == 'stop_monitoring':
                self.performanceMonitoring.stop_monitoring()
                return {'message': 'æ€§èƒ½ç›‘æ§å·²åœæ­¢'}

            elif action == 'enable_slow_query_log':
                await self.configure_slow_query_log(options.get('longQueryTime', 1))
                return {'message': f'æ…¢æŸ¥è¯¢æ—¥å¿—å·²å¯ç”¨ï¼Œé˜ˆå€¼: {options.get("longQueryTime", 1)}ç§’'}

            elif action == 'disable_slow_query_log':
                await self.disable_slow_query_log()
                return {'message': 'æ…¢æŸ¥è¯¢æ—¥å¿—å·²ç¦ç”¨'}

            elif action == 'get_active_slow_queries':
                return await self.slowQueryAnalysis.get_active_slow_queries()

            elif action == 'get_config':
                return await self.get_slow_query_log_config()

            else:
                raise MySQLMCPError(
                    f"æœªçŸ¥çš„æ€§èƒ½ä¼˜åŒ–æ“ä½œ: {action}",
                    ErrorCategory.INVALID_INPUT,
                    ErrorSeverity.MEDIUM
                )
        except Exception as error:
            raise MySQLMCPError(
                f"æ€§èƒ½ä¼˜åŒ–æ“ä½œå¤±è´¥: {str(error)}",
                ErrorCategory.UNKNOWN,
                ErrorSeverity.MEDIUM
            )
